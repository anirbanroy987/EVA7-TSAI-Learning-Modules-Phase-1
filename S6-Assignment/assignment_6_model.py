# -*- coding: utf-8 -*-
"""Assignment_6_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l85AmRjK8lOML-q5GGy4aDcUIF5f3pKb
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torchvision.datasets import MNIST
from collections import OrderedDict
from tqdm import tqdm
import matplotlib.pyplot as plt
import pandas as pd

import numpy as np
import seaborn as sn

def get_norm_layer( norm_layer_type, num_channels, num_groups_for_group_norm=None):
    """
    norm_layer_type: 'batch' | 'group' | 'layer'
    num_channels: # of channels
    """
    if norm_layer_type == "batch":
       
        nl = nn.BatchNorm2d(num_features=num_channels)
    elif norm_layer_type == "group":
        nl = nn.GroupNorm(num_groups=num_groups_for_group_norm, num_channels=num_channels)
    elif norm_layer_type == "layer":
       
        nl = nn.GroupNorm(num_groups=1, num_channels=num_channels)

    return nl

class Net(nn.Module):
    def __init__(self, norm_layer_type, num_groups_for_group_norm=None):
        """
        normalization_type: 'batch' | 'group' | 'layer'
        """
        super(Net, self).__init__()
        
        dropout=0.05
 
        ####### 
        # Convolution Block #1
        #########
        self.conv1 = nn.Sequential(OrderedDict([
            ('conv1', nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1, bias=False)),
            ('relu', nn.ReLU()),
            ('batchNorm2d', get_norm_layer(norm_layer_type, num_channels=8, num_groups_for_group_norm=num_groups_for_group_norm) ),
            ('dropOut2d', nn.Dropout2d(p=dropout))
          ])
        ) # Input=28, Output=28, rf=3

        self.conv2 = nn.Sequential(
            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=1, bias=False),
            nn.ReLU(),
            get_norm_layer(norm_layer_type, num_channels=8, num_groups_for_group_norm=num_groups_for_group_norm),
            nn.Dropout2d(p=dropout)
        ) # Input=28, Output=28, rf=5
 
        self.conv3 = nn.Sequential(
            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=1, bias=False),
            nn.ReLU(),
            get_norm_layer(norm_layer_type, num_channels=8, num_groups_for_group_norm=num_groups_for_group_norm),
            nn.Dropout2d(p=dropout) 
        ) # Input=28, Output=28, rf=10
 
        ####### 
        # Transition Block #1
        #########
        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2) # Input=28, Output=14, rf=6
 
        self.conv4= nn.Sequential(
            nn.Conv2d(in_channels=8, out_channels=12, kernel_size=1, padding=0, bias=False),
        ) # Input=14, Output=14, rf=32
 
        ####### 
        # Convolution Block #2
        #########
        self.conv5 = nn.Sequential(
            nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, padding=1, bias=False),
            nn.ReLU(),
            get_norm_layer(norm_layer_type, num_channels=12, num_groups_for_group_norm=num_groups_for_group_norm),
            nn.Dropout2d(p=dropout)
        ) # Input=14, Output=14, rf=14
 
        self.conv6 = nn.Sequential(
            nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, padding=1, bias=False),
            nn.ReLU(),
            get_norm_layer(norm_layer_type, num_channels=12, num_groups_for_group_norm=num_groups_for_group_norm),
            nn.Dropout2d(p=dropout) 
        ) # Input=14, Output=14, rf=24
        
        self.conv7 = nn.Sequential(
            nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, padding=1, bias=False),
            nn.ReLU(),
            get_norm_layer(norm_layer_type, num_channels=12, num_groups_for_group_norm=num_groups_for_group_norm),
            nn.Dropout2d(p=dropout) 
        )# Input=14, Output=14, rf=24
        
        self.conv8 = nn.Sequential(
            nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, padding=1, bias=False),
            nn.ReLU(),
            get_norm_layer(norm_layer_type, num_channels=12, num_groups_for_group_norm=num_groups_for_group_norm),
            nn.Dropout2d(p=dropout) 
        ) # Input=14, Output=14, rf=24

        self.conv9 = nn.Sequential(
            nn.Conv2d(in_channels=12, out_channels=10, kernel_size=3, padding=1, bias=False),
            nn.ReLU(),
            get_norm_layer(norm_layer_type, num_channels=10, num_groups_for_group_norm=num_groups_for_group_norm),
            #nn.Dropout2d(p=dropout) 
        ) # Input=14, Output=14, rf=24
        
        #######
        # Transition block #2
        #######
        self.maxpool2= nn.MaxPool2d(kernel_size=2, stride=2) # Input=6, Output=3, chan=12, 
 
        ####### 
        # Output Block
        #########
        # global average pool before 1x1 to reduce computation
        #self.global_avgpool = nn.AdaptiveAvgPool2d(output_size=1)  # Input=3, Output=1, chan=12,
        self.global_avgpool = nn.AvgPool2d(kernel_size=10)  # Input=3, Output=1, chan=12,
 
        self.conv10 = nn.Sequential(
            
            nn.Conv2d(in_channels=12, out_channels=10, kernel_size=1, padding=0, bias=False),
        ) # Input=1, Output=1, chan=10, 
    
    def forward(self, x):
        #####
        # conv block #1
        ########
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
 
        #####
        # Transitioni block #1
        ########
        x = self.maxpool1(x)
        x = self.conv4(x)
 
        #####
        # conv block #2
        ########
        x = self.conv5(x)
        x = self.conv6(x)
        x = self.conv7(x)
        x = self.conv8(x)
        x = self.conv9(x)

        #######
        # Transition block #2
        #######
        #x = self.maxpool2(x)
 
        #####
        # output block
        ########
        x = self.global_avgpool(x)        
        #x = self.conv10_16_1_1_10(x)
               
        x = x.view(-1, 10)
        return F.log_softmax(x)